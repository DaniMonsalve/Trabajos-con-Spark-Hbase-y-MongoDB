# -*- coding: utf-8 -*-
"""Spark_SQL_DataFrames.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fUoIbGm0qE7QNTbNnWkr67OE8s5GgVM-
"""

# Instalar SDK Java 8

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Descargar Spark 3.2.2

!wget -q https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz

# Descomprimir el archivo descargado de Spark

!tar xf spark-3.2.3-bin-hadoop3.2.tgz

# Establecer las variables de entorno

import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.3-bin-hadoop3.2"

# Instalar la librería findspark 

!pip install -q findspark

# Instalar pyspark

!pip install -q pyspark

#Inicio una sesion de spark

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

# Crear DataFrames con la funcion .toDF

rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))

rdd.collect()

df = rdd.toDF(['numero', 'cudrado'])

df.printSchema()

df.show()

# Crear un DataFrame a partir de un RDD con schema

rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# Primera vía para crear la estructura

esquema1 = StructType(
    [
     StructField('id', IntegerType(), True),
     StructField('nombre', StringType(), True),
     StructField('saldo', DoubleType(), True)
    ]
)

# Segunda vía

esquema2 = "`id` INT, `nombre` STRING, `saldo` DOUBLE"

df1 = spark.createDataFrame(rdd1, schema=esquema1)

df1.printSchema()

df1.show()

df2 = spark.createDataFrame(rdd1, schema=esquema2)

df2.printSchema()

df2.show()

# Crear un DataFrame a partir de un rango de números

spark.range(5).toDF('id').show()

spark.range(3, 15).toDF('id').show()

spark.range(0, 20, 2).toDF('id').show()

#Antes de nada creo una carpeta nueva, llamada data, en la que cargare los txt
#Se tiene que hacer siempre que se actualice el colab

# Crear un DataFrame mediante la lectura de un archivo de texto

df = spark.read.text('./data/dataTXT.txt')

df.show()

df.show(truncate=False)

# Crear un DataFrame mediante la lectura de un archivo csv

df1 = spark.read.csv('./data/dataCSV.csv')

df1.show()

df1 = spark.read.option('header', 'true').csv('./data/dataCSV.csv')

df1.show()

# Leer un archivo de texto con un delimitador diferente y con cabecera

df2 = spark.read.option('header', 'true').option('delimiter', '|').csv('./data/dataTab.txt')

df2.show()

# Crear un DataFrame a partir de un json proporcionando un schema

from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType #importo las clases que voy a necisitar

json_schema =  StructType(
    [
     StructField('color', StringType(), True),
     StructField('edad', IntegerType(), True),
     StructField('fecha', DateType(), True),
     StructField('pais', StringType(), True)
    ]
)

df4 = spark.read.schema(json_schema).json('./data/dataJSON.json')

df4.show()

df4.printSchema()

# Crear un DataFrame a partir de un archivo parquet

df5 = spark.read.parquet('./data/dataPARQUET.parquet')

df5.show()

# Otra alternativa para leer desde una fuente de datos parquet en este caso

df6 = spark.read.format('parquet').load('./data/dataPARQUET.parquet')

df6.printSchema()

# Primera alternativa para referirnos a las columnas

df5.select('title').show()

# Segunda alternativa para referirnos a las columnas

from pyspark.sql.functions import col

df5.select(col('title')).show()

# select

df = spark.read.parquet('./data/datos.parquet')

df.printSchema()

from pyspark.sql.functions import col

df.select(col('video_id')).show()

df.select('video_id', 'trending_date').show()

#select con una expresión nueva
df.select(
    col('likes'),
    col('dislikes'),
    (col('likes') - col('dislikes')).alias('aceptacion')
).show()

# selectExpr

df.selectExpr('likes', 'dislikes', '(likes - dislikes) as aceptacion').show()
df.selectExpr("count(distinct(video_id)) as videos").show()

# filter
from pyspark.sql.functions import col

df.show()

# filter por id
df.filter(col('video_id') == '2kyS6SvSYSE').show()

## filter con where
df1 = spark.read.parquet('./data/datos.parquet').where(col('trending_date') != '17.14.11')

df1.show()

df2 = spark.read.parquet('./data/datos.parquet').where(col('likes') > 5000)

df2.filter((col('trending_date') != '17.14.11') & (col('likes') > 7000)).show()

df2.filter(col('trending_date') != '17.14.11').filter(col('likes') > 7000).show()

# Transformaciones - funciones distinct y dropDuplicates



# distinct

df_sin_duplicados = df.distinct()

print('El conteo del dataframe original es {}'.format(df.count()))
print('El conteo del dataframe sin duplicados es {}'.format(df_sin_duplicados.count()))

# función dropDuplicates

dataframe = spark.createDataFrame([(1, 'azul', 567), (2, 'rojo', 487), (1, 'azul', 345), (2, 'verde', 783)]).toDF('id', 'color', 'importe')

dataframe.show()

dataframe.dropDuplicates(['id', 'color']).show()

# Transformaciones - funciones sort y orderBy

#Antes de nada creo mi df borrando duplicados

from pyspark.sql.functions import col

df = (spark.read.parquet('./data/datos.parquet')
    .select(col('likes'), col('views'), col('video_id'), col('dislikes'))
    .dropDuplicates(['video_id'])
)

df.show()

# sort

df.sort('likes').show()

from pyspark.sql.functions import desc

df.sort(desc('likes')).show() #ordenando por la columna likes

# función orderBy (es una función más relacional)

df.orderBy(col('views')).show()

df.orderBy(col('views').desc()).show()

dataframe = spark.createDataFrame([(1, 'azul', 568), (2, 'rojo', 235), (1, 'azul', 456), (2, 'azul', 783)]).toDF('id', 'color', 'importe')

dataframe.show()

dataframe.orderBy(col('color').desc(), col('importe')).show()

# funcion limit 

top_10 = df.orderBy(col('views').desc()).limit(10)

top_10.show()

# Transformaciones - funciones withColumn y withColumnRenamed

# withColumn

from pyspark.sql.functions import col

df_valoracion = df.withColumn('valoracion', col('likes') - col('dislikes'))

df_valoracion.printSchema()

df_valoracion1 = (df.withColumn('valoracion', col('likes') - col('dislikes'))
                    .withColumn('res_div', col('valoracion') % 10)
)

df_valoracion1.printSchema()

df_valoracion1.select(col('likes'), col('dislikes'), col('valoracion'), col('res_div')).show()

# withColumnRenamed

df_renombrado = df.withColumnRenamed('video_id', 'id')

df_renombrado.printSchema()

df_error = df.withColumnRenamed('nombre_que_no_existe', 'otro_nombre') #como esa columna no existe no hace nada, tampoco sale ningún error

df_error.printSchema()

# Transformaciones - funciones drop, sample y randomSplit

df = spark.read.parquet('./data/datos.parquet')

# drop

df.printSchema()

df_util = df.drop('comments_disabled')

df_util.printSchema()

df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link')

df_util.printSchema()

df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link', 'cafe')

df_util.printSchema()

# sample

df_muestra = df.sample(0.8)

num_filas = df.count()
num_filas_muestra = df_muestra.count()

print('El 80% de filas del dataframe original es {}'.format(num_filas - (num_filas*0.2)))
print('El numero de filas del dataframe muestra es {}'.format(num_filas_muestra))


#Si quisieramos replicar esa misma muestra utilizamos el parámetro seed como en los siguentes ejemplos
df_muestra = df.sample(fraction=0.8, seed=1234)
df_muestra = df.sample(withReplacement=True, fraction=0.8, seed=1234) #Si queremos que tome esas filas del dataframe con remplazo

# randomSplit

train, test = df.randomSplit([0.8, 0.2], seed=1234)

train, validation, test = df.randomSplit([0.6, 0.2, 0.2], seed=1234)

print(train.count())

validation.count()

print(test.count())

# Trabajo con datos incorrectos o faltantes
df = spark.read.parquet('./data/datos.parquet')

df.count() #cuantos registros tiene el df

df.na.drop().count() #Número de registros no null

df.na.drop('any').count() #Mismo resultado

df.dropna().count() #Mismo resultado

df.na.drop(subset=['views']).count() #Número de registros no null en views

df.na.drop(subset=['views', 'dislikes']).count() #Número de registros no null en views y en dislikes

from pyspark.sql.functions import col

df.orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()

df.fillna(0).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show() #sustituir null por 0

df.fillna(0, subset=['likes', 'dislikes']).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show() #sustituir null por 0 en columnas específicas

# Acciones sobre un dataframe en Spark SQL
df = spark.read.parquet('./data/datos.parquet')

# show

df.show()

df.show(5)

df.show(5, truncate=False)

# take
#La salida será una lista

df.take(1)

# head

df.head(1)

# collect
#Riesgo para la memoria

df.select('likes').collect()

# Escritura de DataFrames

df1 = df.repartition(2) #creo 2 particiones

df1.write.format('csv').option('sep', '|').save('./output/csv1') #cambio el separador,selecciono la carpeta donde guardar en csv de salida, en este caso mi carpeta de entorno

#Si quisiera devolverlo a una sola partición
df1.coalesce(1).write.format('csv').option('sep', '|').save('./output/csv2')

#para guardar el dataframe particionando los datos

df.printSchema()#df original

df.select('comments_disabled').distinct().show() #me centro en la columna comment_disabled

from pyspark.sql.functions import col

df_limpio = df.filter(col('comments_disabled').isin('True', 'False')) #Realizo un filtrado para limpiarlo

df_limpio.write.partitionBy('comments_disabled').parquet('./output/parquet') #los guardo particionando los datos por la columna comment_disabled, una partición para false y otra para true
#crea tantas particiones como valores diferentes tenga la columna seleccionada

# Persistencia de DataFrames


df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'valor'])

df.show()

df.persist() #persistir en memorio

df.unpersist() #para retirar en memoria

df.cache() #Almacenar sólo en memoria


from pyspark.storagelevel import StorageLevel

df.persist(StorageLevel.DISK_ONLY) #almacenar solo en disco

df.persist(StorageLevel.MEMORY_AND_DISK) #tanto memoria como disco